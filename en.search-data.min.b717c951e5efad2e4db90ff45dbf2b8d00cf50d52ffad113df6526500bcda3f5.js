'use strict';(function(){const b={cache:!0};b.doc={id:'id',field:['title','content'],store:['title','href']};const a=FlexSearch.create('balance',b);window.bookSearchIndex=a,a.add({id:0,href:'/docs/changelog/',title:"Changelog",content:"Changelog About This page provides an overview of the most recent changes to the cluster.\nLegend    Mark Meaning     #NAME assigned to person with NAME   ### collaborative task   [DONE] task complete   [NNNN] task could not be completed   [DELAY] task has been delayed   [PART] task has been partly completed    Future Cluster upgrade Note Todo  Due to some strange configuration decision by Bright, systemd-journald logs are not persistent (WTF?!). The fix is simple, change the Storage key-value to persistent in /etc/systemd/journald.conf. numa-devel package (needed for some operations of the hwloc library) is not installed consistently on all nodes!  (Jan 2019) Software Stack upgrade Tasks  General software update (headnode + nodes)  Move to BCM 8.1 [DONE] Nvidia now (Nov. 2018) support RHEL-based distros on the DGX-1 (see http://go.nvidianews.com/h671aUF02O0001EjM900NNk) - using RHEL7 would be a better choice in the long term given that all the other nodes use RHEL7. [DELAY] Upgrade MLNX firmware (not possible on gpu0[1-8] as the ASUS card is OEM-shit!) [PART] Setup CPU-based OpenCL support (using the amd-app-sdk - note that AMD longer supports this, as such max OCL API version support is 1.2) [DONE]   Extension of NetData panel with live status output for users (maybe? definitely not doing this as there are performance related concerns) Local Docker and Singularity Registry  Setup cm-singularity\u0026hellip; [DONE]  actually, we want to compile from source - also testing has revealed that accessing the NV-related libraries stored in /cm/shared via singularity image is non-trivial. Further work is needed, though nothing significant as far as I can tell. There is no stable registry like service yet other than SyLabs Cloud (a Docker Hub contender), which is free but for public images only\u0026hellip;     Mirror NVIDIA Docker Registry (is this really useful?) [DELAY]  Notes The main purpose here is to allow users to use Docker images for their work. Main reason is that several vendors/frameworks provide Docker images that are specially optimised (how, and to what degree, is not known) and these optimisation are not available as binary releases (ugh\u0026hellip;). Additionally some user prefer docker as it simplifies managing their environment and maintains a degree of consistency across machines/distros.\nMain problem: docker is insecure for multi-user environments, has no resource management, and requires additional infrastructure/management. Solution: Singularity (http://singularity.lbl.gov/index.html), which is a tool which converts docker images to binaries - which generate a fs-tree within a chroot. There is no need for a hypervisor or other infrastructure and it works well together with existing management systems like SLURM. Additionally is recommended by Bright Computing (CM provider).\nOf course, the usual system updating should be done. In particular CUDA 9.1 is out, so it might be nice for users - especially on dgx01.\nRDMA setup is not working correctly as the ib_srp kernel module has some strange version mismatch meaning it will not load into the kernel\u0026hellip; this is not critical though as SRP protocol is used for accessing SCSI-related resources on a remote system\u0026hellip; as far as I know no-one needs this feature.\n(Oct 2017) DGX-1 Upgrade Ubuntu 14 to 16 Tasks  CM 8.0 Upgrade [DONE]  Reset GPU/Intel node images (maybe?) [DONE]   DGX-1  Upgrade to latest image [DONE] \u0026mdash; //thank you Rob Stewart// Upgrade BMC to latest firmware [DONE] \u0026mdash; //thank you Rob Stewart// Get CM 8.0 provision working (maybe?) [DELAY] Setup Slurm-client [DONE] Setup/Test NVIDIA Registry [PART]  See how well this works with Singularity [PART]     Extend Wiki to include Singularity Examples [DELAY] Extend salloc block to nodes [DONE] Upgrade to CUDA 9.0 [DONE]  Upgrade to cuDNN v6 and v7 [DONE]    Notes  DGX-1  NVIDIA have released Ubuntu 16 image for the box - which supports the latest version of SLURM, meaning we can now setup batch managment control there. After DGX-UG meeting, Docker based system seems like a nice solution to reduce lag in installing user modules/keeping the modules system up to date \u0026mdash; using EasyBuild is aweful. We cannot setup docker as its insecure (allows for privilage esciliation from within the container), but instead we can use Singularity which converts a Docker image into set of scripts which generated a fake-chroot. No privilages are needed for this + its easier to gain access to the hardware as there isn\u0026rsquo;t a hypervisor. Users can just extend/create a sbatch script which launched their Singularity image as such.   CM 8.0 is out, with the primary update being to support more Linux Distros as slave-nodes - which is fanatastic because it means we could potentially provision the DGX-1 using NVIDIA image - Whoo!!! Other then that they hve updated their data gathering and monitoring system (to what effect I don\u0026rsquo;t know), and a new snazzy web-based management interface.  (Nov 2016) Cluster RHEL6 to RHEL7 upgrade Tasks  Configure default account settings (simplifies users initial experience) [DONE]  Create informative banner + motd [DONE] Set default modules (e.g. shared) [DONE]   Install additional packages  GCC (4.8 and 6.1) + glibc 2.17 [DONE] Intel Compiler Suite  Intel VTune facility   Portland Compiler Suite [DONE] Python + pip (versions 2.7 and 3.4) [DONE]   Configure cluster management system  Adaptive user access to nodes (SSH) [DONE] Add MIC hosts and MIC nodes  identify MIC processor units     Configuration of SLURM  GPU types setup (mainly for gpu08) [DONE] Create dedicated MIC queue Initiate Usage logging facility (needed in order to log the number of hours, cpu cores, gpus, etc. used by a user or job) #Igor   Configure Nodes  Head Node  Propagate sudoers file to nodes using systemd timers and rsync [DONE] Apcupsd for APC monitoring [DONE]  Setup notification [DONE] //not necessary as default settings are adequate//   Configure SNMP for compute nodes (from IPMI) - this allows use to receive hardware level notification outwith the OS   MIC Nodes  Setup the MIC Node image (copy from default image) #Igor Configure MICs relevent #Igor  install the MIC toolchain #Igor compile dedicated linux kernel #Igor   InfiniBand firmware update [DONE]  install OFED framework [DONE]     GPU Nodes  Setup GPU node image [DONE]  install cuda-driver [DONE]   InfiniBand firmware update  Currently running OEM //ConnectX 2 VPI// firmware   install OFED framework [DONE]     Update Intranet Wiki  New usage pattern for nodes (user can now SSH to node IFF they allocate the node, e.g. salloc -N 1). This allows users to now directly interact with a node, monitor processes, and in some cases install there own software. Be aware, the node is stateless, thus on a weekly cycle it will be re-installed and all data there will be lost. #Hans Provide more examples on using SLURM in general - with support for multiple GPU types, the user can now explicitly select the K20 or Quadro 6000. (Internal pages) Include description of cluster upgrade process as well as custom configurations. ###    Backburner  Setup YARP Setup MatLab clustering  Notes InfiniBand  It is not clear yet if the switch is acting as the sub-net manager or if one needs to be setup (e.g. opensm). Though the firmware versions of the IB adaptors are usable for OFED 3.4 from Mallonex, there seems to be a slight problem with the adaptors install on the GPU nodes. These are not original Mallonex adaptors, but some OEM ones (probably ASUS, but maybe HP). As such their device_id is not recognised (actually is unparsable) by the OFED management tools/services. This so far is not problematic, but could cause problems if an application needs specific features the OEM firmware is lacking. kernel version locked, due to how the OFED is setup, any updates to the kernel require a complete re-install of the OFED. yum is already setup to ignore kernel package updates.  Node Imaging  The tools/procedures to do node imaging is rather unrefined, and open to errors. In essences, though the CM systems has a notion of variants on images, it provides no means of merging these together to create one. Instead on needs to keep moving forward (which risks missing out on packages updates). Do not do anything in parallel - work cannot be shared when dealing with chroot an image.  APCupsd  The default settings of apcupsd are adequate. In future, if further action is wanted, we can extent the functionality by adding scripts to /etc/apcupsd (see /etc/apcupsd/apccontrol for examples).  SLURM GPU08 (or having multiple GPUs)  The CMD way of setting things up is through Roles, which describe certain properties. These are interpreted and result in configuration files on the node being populated (in this instance, /etc/slurm/slurm.conf and /etc/slurm/gres.conf). The relevant properties are  gpus gpudevices and gres   These properties though are not what we want, which is to setup GRES Types, in order to distinguish between GPU types. Setting gpus indicates the number of GPUs we have, but not their type. Setting gres allows use to set an arbitrary GRES type, which not useful here. And finally we have gepudevices; this allows use to enter in a list of GPU devices as path references (e.g. /dev/nvidia0). A funny thing about this property is that it no interpreted or sanitised in any way - we can right anything here we want. What we want is:  Name=gpu File=/dev/nvidia0 Type=tesla Name=gpu File=/dev/nvidia1 Type=quadro  The gpudevices property fills the space in \u0026lt;\u0026gt;, e.g. Name=gpu File=\u0026lt;\u0026gt; So to get something like above we do  gpudevices equals /dev/nvidia0 Type=tesla\\nName=gpu File=/dev/nvidia1 Type=quadro; notice the \\n (newline character).   This is a hack, this must be checked if we ever upgrade! There is no other way of doing this sadly.  "}),a.add({id:1,href:'/docs/compilers/',title:"Compilers",content:"Compilers On the cluster we offer several C/C++/Fortran compilers through our modules system.\nBelow is given a short introduction to these and a few tips on how best to utilise them.\nGNU Compiler Collection Intel Compiler Suite Portland Group Compiler Suite The best resource on using any of the PGI compilers is by reading their user guide, which can be found at http://www.pgroup.com/resources/docs.htm.\nAdvice and Tips Compiler Flags The PGI developers recommend that a few key flags be active for all compilations, specifically -fast -Mipa=fast,inline. The reasons for this and other details can be found on https://www.pgroup.com/support/compile.htm.\nUser Configurations The PGI compiler suite has a default set of configurations that it uses when first loaded. These can be altered, or new configurations added. The main benefit of this is that certain compilation stages can be influenced in ways which are not covered by command line arguments.\nThese configurations can be set by creating and inserting the configuration settings into the appropriate RC file within your home directory. The name of such a file must be of the format .\u0026lt;compilerbin\u0026gt;rc, e.g. for pgcc the RC file is .pgccrc.\nFor example, PGI does not support the -pthread flag, and will error out if it encounters this flag. For some situations, using the -lpthread flag would be the solution - but other situations do not allow you to make this change. Instead, a filter can be set within the PGI configuration which checks for the -pthread flag and replaces it with the -lpthread flag. The configuration to achieve this is switch -pthread is replace(-lpthread) positional(linker);.\n"}),a.add({id:2,href:'/faq/',title:"FAQ",content:"Frequently Asked Questions Information about the Cluster  What are the available resources on the cluster? A table showing the hardware specifications for all nodes is provided here. Who maintains the cluster? The cluster is administrated by Hans Viessmann (you can contact him via the contact form).  Getting Help  What help/support is available? No formal support is provided, other than basic adminstrative tasks and maintanence work. Users may contact the administrator for help, such as in the case of install/updating some hardware and/or software. Requests to create custom environments or other specialised tasks are not acceptable.  "}),a.add({id:3,href:'/register/',title:"Register",content:"Register To register, please read the Terms and Conditions and then fill out the form below and submit it. You should receive a response on the same day (within working hours) or latest next day.\nNOTE: not all requests to access the cluster will be accepted! Please consider the following questions, and use these as a guide to determine whether or not your reason to use the cluster is justified or appropriate:\n Does my application need access to high-performance/specialised hardware (e.g. NVIDIA K20, Intel Xeon Phi, etc.)? Does my application need to be run on multiple nodes (e.g. via MPI)? Is the software stack on the cluster appropriate for my application (e.g. SLURM batch management)? How long will the runtime of my application be (does it take an hour, or a day, or multiple weeks)?  External Access: Heriot-Watt University impose very strict policies on their computer networks, specifically that critical hardware infrastructures are not exposed to the outside world. Thus, to access the cluster from outside the University requires that users make use of the University VPN \u0026mdash; members of staff (including PhD students) have access to this service already, everyone else will need to specifically request this. Please indicate this in the form below.\nLoading‚Ä¶ "}),a.add({id:4,href:'/docs/the-cluster/',title:"The Cluster",content:"The Cluster The cluster is made up of 1 head node and 10 compute nodes. Of the compute nodes, 8 are AMD based systems while the other 2 are Intel based systems. Here is the following hardware stack for all of the nodes:\nRepairs The DGX1 node is currently unavailable due to faulty GPU board. There is no ETA on when this will be repaired.     Node Type Node Name Processor RAM HDD (/tmp) Extra Components     Head Node robotarium AMD Opteron 6320 (8 cores) 64 GB 916 GB \u0026mdash;   AMD Compute Node gpu01, gpu02, gpu03, gpu04, gpu05, gpu06 4x AMD Opteron 6376 (64 cores) 512 GB 40 GB NVIDIA K20 GPU, (2x in gpu06)   AMD Compute Node (Large RAM) gpu07, gpu08 4x AMD Opteron 6376 (64 cores) 1024 GB 40 GB NVIDIA K20 GPU (only in gpu07), 2x NVIDIA Titan Xp GPUs (only in gpu08)   Intel Compute Node mic01, mic02 2x Intel Xeon E5-2650v2 (16 cores) 128 GB 40 GB \u0026mdash;   NVIDIA DGX1 Node dgx01 2x Intel Xeon E5-2698v4 (40 cores) 512 GB 440 GB (7 TB at /raid/scratch) 8x NVIDIA Tesla P100    In addition to the above listed nodes, there are also some special-purpose nodes \u0026mdash; specifically, the cluster has eight Intel MIC nodes:\n   Node Type Node Name Processor Onboard Memory Extra Components     Intel MIC Compute Units mic01-0, mic01-1, mic01-2, mic01-3, mic02-0, mic02-1, mic02-2, mic02-3 Intel Xeon Phi 5120D 8 GB \u0026mdash;    All of the nodes are connected by both a 1Gbps network and an InfiniBand 4x 10Gbps network. We support the full OFED stack.\nThe nodes are made available through the SLURM resource management system \u0026mdash; more information on queues can be found here.\nDetails about the OS and available software packages can be found on the software information page.\n"}),a.add({id:5,href:'/',title:"About",content:"About This is the internal documentation site for the ECR Robotarium Cluster at Heriot-Watt University. It is funded by the Centre for Doctoral Training in Robotics and Autonomous Systems and the Edinburgh Centre for Robotics.\nThe site provides the following information and facilities:\n How to register to gain access to the cluster How to use the cluster FAQ \u0026amp; support contact details The terms and conditions  Note: this site is still incomplete, more information will be added when it is needed. The most helpful thing to do is to send us questions, and we will post the response to those questions here.\nReferencing The cluster can be referenced within your publication or output directly via a DOI number, please see the here for BibTeX referencing details.\n"}),a.add({id:6,href:'/docs/',title:"Documentation",content:"Documentation Here we provide a few pointers on how to use the cluster facilities. It is expected that the user has a good knowledge of *UNIX-based systems and can use a command line.\nYou can access the various topics via the following buttons: Cluster Layout  Quickstart Guide  How-To Guide  "}),a.add({id:7,href:'/docs/how-to/',title:"How To",content:"How To Here we provide some guides for various aspects of using the Robotarium cluster. The most important ones are listed directly below \u0026mdash; it is a good idea to familiarise yourselves with these first. Please be aware that some of the guides here are hosted on other sites and will need you to navigate away from here.\nNote: this page is still being populated, if you feel that anything is missing or amiss please contact us and we\u0026rsquo;ll update this page appropriately.\nBasics Accessing the Cluster The cluster can only be accessed from within the Heriot-Watt University network. Users wishing to use it from outside the university will need to setup an SSH tunnel (see SSH Forwarding) or use the HW-VPN (please contact us to give you access). HW staff have VPN access per default (see the HW-VPN details).\n Notice There is a user portal website available for users to check the current load of the cluster, you\u0026rsquo;ll need to login using your cluster account to access this. For the moment, the site is using a self-signed SSL certificate, as such you will likely need to set your browser to accept the certificate before you can access the site.\n SSH Forwarding  Notice This is only available for users with MACS (School of Mathematics and Computer Science) accounts. Otherwise you can try out the HW-VPN.\n As a quick introduction, a SSH tunnel is a secure method to transport other network protocols across network boundaries \u0026mdash; assuming of course that you can access the network over SSH in the first place. This is achieved by establishing a SSH connection between the client (yourself) and the remote server, and then encapsulating (tunnelling) other network protocols \u0026mdash; such as HTTP(S), FTP, and SSH \u0026mdash; into it. In this instance, we will use a tunnel to forward an SSH connection from inside the university network to your system.\nThe basic idea is that you can forward SSH connections, meaning that one tunnels a series of SSH connections over one or more relay hosts to connect to some server. Historically, there are several way of doing this (if your interested you can read about it here), but the easiest way to achieve this is by using the SSH ProxyCommand option and the -W flag. Together these provide a way to chain SSH connections together. An example of this on the command line would be:\n$ ssh -o ProxyCommand='ssh -W %h:%p \u0026lt;USER\u0026gt;@\u0026lt;IP of remote server\u0026gt;' \u0026lt;USER\u0026gt;@\u0026lt;IP of relay server\u0026gt; Within the ProxyCommand we specify the remote server and through the -W flag we state that we want the connection to be forwarded to the relay server \u0026mdash; this is given as the last argument of the SSH command.\nWe can simplify this by removing the need to write all that out by placing it the ~/.ssh/config file. The structure of this would then be:\nHost SERVER1 HostName 192.168.0.1 User someone IdentityFile ~/.ssh/id_rsa Host SERVER2 HostName 192.168.1.1 User someoneelse ProxyCommand ssh SERVER1 -W %h:%p  Then by calling ssh SERVER2 you automatically get the connection forwarded over SERVER1.\nSetting up Your Account The Module System The cluster provides many different software packages (see Software for details) through the Modules system. The basic commands to access and manage these software packages is:\n module list \u0026mdash; list loaded modules module avail \u0026mdash; list all available modules module load \u0026lt;module name\u0026gt; \u0026mdash; load a specific module module unload \u0026lt;module name\u0026gt; \u0026mdash; unload a specific module  Further commands can be found in the man-page \u0026mdash; man module.\nWhen you first login, you\u0026rsquo;ll find that the default-environment module has been loaded \u0026mdash; this gives you access the SLURM batch-management system. To make changes to what modules are loaded automatically for you, use module initadd and module initrm to add entries for you. It is advised not include module purge within your .bashrc file unless you know what you are doing.\nMore details on how to use the modules system can be read up in the Modules section.\nQueueing System The cluster uses SLURM, or the Simple Linux Utility for Resource Management to manage user workloads. It is the only means by which to run applications on the cluster. A good resource to understand how to use it is through the quickstart guide.\nThe Queues In the table below is given the queues (or as they are referred to in SLURM \u0026mdash; the partitions). The queues are ordered in their priority, with amd-shortq having the highest priority. What this means is that jobs assigned to that queue will likely be allocated before jobs in other queues.\n   Name Time Limit Nodes Notes     amd-shortq 1 hour gpu01 default queue ‚Äì please take note of this!   amd-longq 7 days gpu02-gpu08    intel-shortq 1 hour mic01    intel-longq 7 days mic02, dgx01    specialq 30 days gpu01-gpu08, mic01-mic02, dgx01 only accessible on request!    If users need access to the specialq or have other needs, please contact us.\nMored details about our queues (partitions) can be found using sinfo. For example:\n# to get detailed information about the queues (include generic resources) $ sinfo -o \u0026#34;%15N %10c %10m %25f %24G\u0026#34; --partition=amd-shortq NODELIST CPUS MEMORY AVAIL_FEATURES GRES gpu07 64 1018366 gpu-host gpu:k20:1,gpu:k6000:1 gpu08 64 1018854 gpu-host gpu:xp:1 $ sinfo -o \u0026#34;%15N %10c %10m %25f %24G\u0026#34; --partition=amd-longq NODELIST CPUS MEMORY AVAIL_FEATURES GRES gpu[01-05] 64 515989 gpu-host gpu:k20:1 gpu06 64 515989 gpu-host gpu:k20:2 $ sinfo -o \u0026#34;%15N %10c %10m %25f %24G\u0026#34; --partition=intel-longq NODELIST CPUS MEMORY AVAIL_FEATURES GRES dgx01 80 515896 gpu-host gpu:p100:8 mic02 32 128906 (null) (null) Which jobs are currently running and which jobs are currently queued for execution can be inspected using the squeue command.\nRunning Programs The three most important SLURM commands for running code are srun, sbatch and scancel. They allow you to insert programs into the queues and to take them off queues again. They also allow you to specify your program\u0026rsquo;s exact needs.\n Notice This is a heterogeneous cluster! Not all codes can be run on all nodes! The nodes gpu01-gpu08 have AMD-based CPUs, the nodes mic01-mic02 have INTEL-based CPUs. Both are i86 compatible but depending on the level of compiler optimisation your programs may only run on one of these two architectures. Furthermore, your code may or may not expect a certain number or even version of GPU or a MIC to be available. If so, you need to make sure that you specify your needs as precisely as possible; otherwise, your code will fail to run or suffer poor performance!\n The four most important needs you can specify are:\n the number of nodes you want --nodes=\u0026lt;n\u0026gt; which nodes you do or do not want --nodelist=\u0026lt;name,name,...\u0026gt; / --exclude=\u0026lt;name,name,...\u0026gt; how many CPUs you want -c\u0026lt;n\u0026gt; which resources (GPUs) you want --gres=\u0026lt;gres,gres,...\u0026gt;  Example usage:\n# to run a command and have its output printed in the shell we use the `srun\u0026#39; command $ srun \u0026lt;...command...\u0026gt; \u0026lt;...args...\u0026gt; # another similar example to run an application on an AMD node in the `amd-longq\u0026#39; queue: $ srun --partition=amd-longq \u0026lt;...command...\u0026gt; \u0026lt;...args...\u0026gt; # or on three AMD nodes, but not on gpu04 or gpu05 $ srun --partition=amd-longq --nodes=3 --exclude=gpu04,gpu05 \u0026lt;...command...\u0026gt; \u0026lt;...args...\u0026gt; # if you want to use all cores on a two AMD nodes (2 nodes with 64 cores each!) $ srun --partition=amd-longq --nodes=2 -c64 \u0026lt;...command...\u0026gt; \u0026lt;...args...\u0026gt; # if you want to use two particular AMD nodes $ srun --partition=amd-longq --nodelist=gpu06,gpu07 \u0026lt;...command...\u0026gt; \u0026lt;...args...\u0026gt; # Note here that the use of `nodelist\u0026#39; implies a minimum number of nodes while # `exclude\u0026#39; does not impact on the number of nodes asked for! # If you want to use one AMD core with a single gpu for longer than 1 hour but you do not care # which node you use $ srun --partition=amd-longq --gres=gpu \u0026lt;...command...\u0026gt; \u0026lt;...args...\u0026gt; # Note here, that this blocks the gpu from being used by anyone else; so # please do only specify `--gres=gpu\u0026#39; if your code *actually does use* a gpu! #If you want to use a system that requires two \u0026#39;K20` gpus for less than 1 hour: $ srun --gres==gpu:k20:2 \u0026lt;...command...\u0026gt; \u0026lt;...args...\u0026gt; # to run in \u0026#39;batch\u0026#39; mode, we use the `sbatch\u0026#39; command # these examples are the same as those above sbatch --output=outfile \u0026lt;...command...\u0026gt; \u0026lt;...args...\u0026gt; sbatch --partition=amd-longq --output=outfile \u0026lt;...command...\u0026gt; \u0026lt;...args...\u0026gt; sbatch --partition=amd-longq --nodes=3 --exclude=gpu04,gpu05 --output=outfile \u0026lt;...command...\u0026gt; \u0026lt;...args...\u0026gt; sbatch --partition=amd-longq --nodes=2 -c64 --output=outfile \u0026lt;...command...\u0026gt; \u0026lt;...args...\u0026gt; sbatch --partition=amd-longq --nodelist=gpu06,gpu07 --output=outfile \u0026lt;...command...\u0026gt; \u0026lt;...args...\u0026gt; sbatch --partition=amd-longq --gres=gpu --output=outfile \u0026lt;...command...\u0026gt; \u0026lt;...args...\u0026gt; sbatch --gres=gpu:k20:2 --output=outfile \u0026lt;...command...\u0026gt; \u0026lt;...args...\u0026gt; # to view the current cluster usage we can look at the queues using `squeue\u0026#39; $ squeue # to cancel a batch job we can use the `scancel\u0026#39; command $ scancel \u0026lt;...job ID...\u0026gt; Modules Most of the software packages available are managed through a system of modules which contain both the software files and configuration information. These modules can be dynamically loaded and unloaded allowing for a great deal of flexibility \u0026mdash; this is especially useful for making use of different versions or builds of the same software. More information can be found on the project website.\nExample usage: $ module avail acml/gcc/64/5.3.1 acml/gcc/fma4/5.3.1 # and many many more modules $ module load cuda65/toolkit # this loads the CUDA SDK and toolkit $ module unload cuda65/toolkit # this unloads the module\nPersonal Modules It is possible to create one\u0026rsquo;s own modules. The benefit of doing this is that the module system will handle your environment variables for you, as well as other configuration. Additionally, if for instance you have a dependency on a module for a piece of software to work, this can be encoded into the module file.\nThe first step is to create a .modulerc file your home directory, e.g. ~/.modulerc. File should contain the following:\n#%Module -*- tcl -*- ## get extra modules files... module use /home/\u0026lt;USERNAME\u0026gt;/.modules Replace \u0026lt;USERNAME\u0026gt; with your username. The module use directive points to a directory where all of your modules are to be found. The next step is to create a module. Assuming that you have created the ~/.modules directory, you can add a module file to the directory. The typical convention is to create a directory naming the software (e.g.~/.modules/mysoftware) and give the module file the version of the software as its name, e.g. ~/.modules/mysoftware/1.0.0.\nAn example of the content of a module file goes as follows:\n#%Module -*- tcl -*- # Helpful messages proc ModulesHelp { } { puts stderr \u0026#34;This module sets up access to something\u0026#34; } module-whatis \u0026#34;sets up access to something\u0026#34; prereq somethingelse # ensure that this module is loaded before hand conflict thatothermodule # ensure that this module is NOT loaded module load gcc # you can have the module load dependencies for you set root /home/\u0026lt;USERNAME\u0026gt;/install/location # a TCL variable setenv SOMEVERION 0.95 # set an environment variable append-path PATH $root/bin # append to $PATH append-path MANPATH $root/man # append to $MANPATH append-path LD_LIBRARY_PATH $root/lib # append to $LD_LIBRARY_PATH For more information on what to put in a module file, have a look at the man pages, e.g. man modulefile.\n"}),a.add({id:8,href:'/sitemap/',title:"Sitemap",content:"Sitemap This page provides an index for all of the pages on this site.\n"}),a.add({id:9,href:'/docs/quick-start/',title:"Quick Start",content:"Quick Start This guide provides a quick-n-dirty series of steps to get you started on the cluster. The guides assumes the following:\n you have an account on the cluster you have experience with Linux-based systems (especially the command line) you have a SSH public-private key pair  Alright then, lift off!\nAccessing the Cluster (SSH) Open up your favourite terminal and type the following:\n$ ssh \u0026lt;username\u0026gt;@robotarium.hw.ac.uk You can only access the cluster while inside the Heriot-Watt University network. If you need access from outside the university, you can request VPN access thourgh the contact form.  You\u0026rsquo;ll be asked to enter in your password, which was provided to you when you signed up.\nSetting up SSH-key access TBC\n"}),a.add({id:10,href:'/terms/',title:"Terms",content:"Terms and Conditions Mains Points  No guarantee of service or quality is given; the cluster is provided as a free service to members and affiliates of the Edinburgh Centre for Robotics with regular maintenance done by a small group of volunteers. Nonetheless some effort is made to ensure continuous uptime and availability. Very limited support is given; we only have a few volunteers at hand to help, and who themselves are busy doing their own work. No backups are made of user data; users are expected to keep copies (on other systems or storage devices) of their work, we take no responsibility for any loss of user data! No guarantees are made regarding the availability of hardware resources; the cluster is setup in a first-come first-serve fashion, as such it is possible that you might find hardware resources that you want to use allocated to another user. The hardware resources are managed through a batch-queue system; any attempt at bypassing it is strictly forbidden and will result in your account being deactivated (possibly permanently). Any work done with the cluster that leads to a publication, or other output, must acknowledge it within that publication (and ideally reference it as well)!  These terms are subject to change at anytime, please check this page regularly!\nAcknowledgement Any use of the cluster that leads to a publication or other output must acknowledge the cluster as a resource within the output. A possible formulation of this for a conference article is:\n ‚Ä¶ we acknowledge our use of the Edinburgh Centre for Robotics' Robotarium Cluster located at Heriot-Watt University, provided through funding by Engineering and Physical Sciences Research Council (EPSRC) Centre for Doctoral Training in Robotics and Autonomous Systems through grant EP/L016834/1 ‚Ä¶\n Referencing In addition to the acknowledgement, you can directly reference the cluster as a resource for your publication:\n@misc{edinburgh_centre_for_robotics_2014_1455754, author = {Edinburgh Centre for Robotics}, title = {Robotarium Cluster}, month = nov, year = 2014, note = {{The cluster is part of the EPSRC Centre for Doctoral Training in Robotics and Autonomous Systems (RAS) in Edinburgh grant (EP/L016834/1) funded by The Engineering and Physical Sciences Research Council (EPSRC) (UK).}}, doi = {10.5281/zenodo.1455754}, url = {https://doi.org/10.5281/zenodo.1455754} } More details about the DOI registration can be at https://doi.org/10.5281/zenodo.1455754.\n"}),a.add({id:11,href:'/contact-us/',title:"Contact Us",content:"Contact Us If you can not find the answers you are looking for on this site or Google, please fill out the form below and we\u0026rsquo;ll try to answer your query as soon as possible.\nSlack Chat The easiest way to get help from existing users and admin-staff is to use Slack. You will need to be invited to join the chat, so please fill out the form below and we\u0026rsquo;ll invite you üòÑ.\nLoading‚Ä¶ "}),a.add({id:12,href:'/categories/',title:"Categories",content:""}),a.add({id:13,href:'/publications/',title:"Publications",content:"Publications Here are listed a few publications associated with the cluster. If you would like to have your publications listed here, please use the contact form. Please don\u0026rsquo;t forget to reference the cluster.\n2021 Artjoms ≈†inkarovs, Hans-Nikolai Vie√ümann, and Sven-Bodo Scholz. Array Languages Make Neural Networks Fast. ARRAY 2021. Link: https://doi.org/10.1145/3460944.3464312\n2020 Hans-Nikolai Vie√ümann and Sven-Bodo Scholz. Efective Host-GPU Memory Management Through Code Generation. IFL 2020. Link: https://doi.org/10.1145/3462172.3462199\n2019 Artjoms ≈†inkarovs, Hans-Nikolai Vie√ümann, Sven-Bodo Scholz. 2019. Array Languages Make Neural Networks Fast. arXiv.org. Link: https://arxiv.org/abs/1912.05234v1\nIgor Shalyminov, Sungjin Lee, Arash Eshghi, Oliver Lemon. Data-Efficient Goal-Oriented Conversation with Dialogue Knowledge Transfer Networks. EMNLP 2019. Link: https://arxiv.org/abs/1910.01302\nIgor Shalyminov, Sungjin Lee, Arash Eshghi, Oliver Lemon. Few-Shot Dialogue Generation Without Annotated Data: A Transfer Learning Approach. SigDial 2019. Link: https://www.aclweb.org/anthology/W19-5904.pdf\nMatias Alejandro Valdenegro Toro. ‚ÄúDeep Neural Networks for Marine Debris Detection in Sonar Images.‚Äù PhD Thesis, Heriot-Watt University, Edinburgh, UK, 13 May 2019. Link: https://arxiv.org/abs/1905.05241\n2018 Hans-Nikolai Vie√ümann, Artjoms ≈†inkarovs, and Sven-Bodo Scholz. Extended Memory Reuse: An Optimisation for Reducing Memory Allocations. IFL 2018. Link: https://doi.org/10.1145/3310232.3310242\nIgor Shalyminov, Arash Eshghi, and Oliver Lemon. Multi-Task Learning for Domain-General Spoken Disfluency Detection in Dialogue Systems. SemDial 2018. Link: http://semdial.org/anthology/papers/Z/Z18/Z18-3008/\nIgor Shalyminov, Ondrej Dusek, and Oliver Lemon. Neural Response Ranking for Social Conversation: A Data-Efficient Approach. Search-Oriented Conversational AI, an EMNLP 2018 Workshop. Link: https://www.aclweb.org/anthology/W18-5701/\n2017 Arash Eshghi, Igor Shalyminov, and Oliver Lemon. Bootstrapping incremental dialogue systems from minimal data: the generalisation power of semantic grammars. EMNLP 2017. Link: https://www.aclweb.org/anthology/D17-1236/\nIgor Shalyminov, Arash Eshghi, and Oliver Lemon. Challenging Neural Dialogue Models with Natural Data: Memory Networks Fail on Incremental Phenomena. SemDial 2017. Link: http://semdial.org/anthology/papers/Z/Z17/Z17-3016/\nIoannis M. Chalkiadakis. \u0026ldquo;TRustNN: towards Building Trust InLSTM Networks for an Emotionrecognition Task, through Data-Driven,Interpretable Visualizations.\u0026rdquo; MSc Thesis, Heriot-Watt University, Edinburgh, UK, 2017.\n"}),a.add({id:14,href:'/tags/',title:"Tags",content:""})})()